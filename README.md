# Executive Summary
In this repository, we seek to compare the performance of multiple machine learning models in music genre classification by utilizing the popular GTZAN dataset, which contains 100 audio files of 30 seconds each - each pertaining to one of 10 music genres (classes). 

Music genre classification has a multitude of potential real-world use cases - specifically in music streaming sites and applications that utilize recommendation systems. We seek to determine the best models for this application. 

Let us visualize our data.

# Data Visualization
Here is a sample audio snippet from the dataset:
https://user-images.githubusercontent.com/78565736/208314633-c5968fc6-d5d0-4544-87e7-cb5e2bdac817.mp4

Here is that same audio file visualized as a sound wave:
![jazz_sound_wave](https://user-images.githubusercontent.com/78565736/208314679-496fe05a-49e1-462e-934a-f09604497b8d.png)

Here is that same sound wave converted to a Mel Spectrogram:
![jazz_melspectrogram](https://user-images.githubusercontent.com/78565736/208314770-b0526f7c-c769-42c9-b30a-57ce2307a2ab.png)

# Methodology and Notebook Explanations
We implemented, trained, and tested a variety of models in order to compare their performance.  


